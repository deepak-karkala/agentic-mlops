# Streaming Architecture Guide: Real-time Agent Reasoning & Workflow Progress

## Overview

This document provides a comprehensive guide to the **Streaming "Reason Cards" & SSE Resilience** implementation in the Agentic MLOps platform. This feature enables real-time visibility into AI agent decision-making processes through a Claude-style unified interface with inline workflow display.

The implementation uses Server-Sent Events (SSE) to stream agent reasoning, workflow progress, and system events in real-time, providing users with transparent insight into AI decision-making processes. The system features an **integrated worker architecture** where the API server and job processor run in the same process, eliminating the need for HTTP-based event bridging and enabling direct streaming service access.

### Key Architecture Features

- **Integrated Worker**: API server and job processor in single process
- **Direct Streaming**: No HTTP bridge needed for event emission
- **LangSmith Integration**: Full observability with run_id correlation
- **Multi-mode Streaming**: LangGraph "updates" and "messages" modes
- **SSE Resilience**: Automatic reconnection with exponential backoff
- **Memory Management**: Event limits and cleanup to prevent memory leaks

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [End-to-End Process Flow](#end-to-end-process-flow)
3. [Backend Implementation](#backend-implementation)
4. [Frontend Implementation](#frontend-implementation)
5. [Data Models & Types](#data-models--types)
6. [Testing & Quality Assurance](#testing--quality-assurance)
7. [Deployment & Configuration](#deployment--configuration)
8. [Usage Examples](#usage-examples)
9. [Troubleshooting](#troubleshooting)
10. [Future Enhancements](#future-enhancements)

## Architecture Overview

```mermaid
graph TB
    User[ðŸ‘¤ User] --> ChatUI[ðŸ–¥ï¸ Enhanced Chat Interface]
    ChatUI --> AsyncAPI[ðŸ”— /api/chat/async]
    AsyncAPI --> JobQueue[ðŸ“‹ Job Queue - Database]

    subgraph "Integrated Worker Process"
        JobQueue --> Worker[âš™ï¸ Integrated Worker]
        Worker --> LangGraph[ðŸ”„ LangGraph Multi-Mode Streaming]
        LangGraph --> UpdatesStream[ðŸ“Š Updates Stream]
        LangGraph --> MessagesStream[ðŸ’¬ Messages Stream]
        UpdatesStream --> EventProcessor[âš¡ Event Processor]
        MessagesStream --> EventProcessor
    end

    subgraph "Direct Streaming Pipeline"
        EventProcessor --> StreamService[ðŸ“¡ Streaming Service]
        StreamService --> EventStorage[ðŸ’¾ In-Memory Events]
        StreamService --> SSEEndpoint[ðŸŒŠ /api/streams/{decision_id}]
        SSEEndpoint --> SSEHook[ðŸ”„ useStreamingEvents Hook]
    end

    subgraph "Real-time UI Updates"
        SSEHook --> WorkflowContainer[ðŸŽ¨ Workflow Container]
        WorkflowContainer --> ProgressHeader[ðŸ“Š Progress Header]
        WorkflowContainer --> WorkflowSteps[ðŸ“‹ Workflow Steps]
        WorkflowSteps --> ReasonCards[ðŸ§  Expandable Reason Cards]
        WorkflowContainer --> HITLGate[â“ HITL Questions]
    end

    subgraph "Observability & State"
        LangGraph --> LangSmith[ðŸ” LangSmith Tracing]
        Worker --> StructuredLogs[ðŸ“ Structured Logging]
        JobQueue --> Database[(ðŸ—„ï¸ Database - Jobs/DecisionSets)]
        LangGraph --> Checkpoints[ðŸ’¾ LangGraph Checkpoints]
    end

    subgraph "Event Types"
        StreamService --> NodeEvents[ðŸŽ¯ Node Start/Complete]
        StreamService --> ReasonCardEvents[ðŸ§  Reason Cards]
        StreamService --> WorkflowEvents[ðŸ”„ Workflow State]
        StreamService --> HITLEvents[â“ HITL Questions]
        StreamService --> ErrorEvents[âŒ Error Events]
        StreamService --> HeartbeatEvents[ðŸ’“ Heartbeat]
    end
```

## End-to-End Process Flow

### Phase 1: User Input & Job Creation

1. **User Action**: User types MLOps requirements in the enhanced chat interface
   ```
   User Input: "I need an ML platform for image classification with high availability requirements and enterprise-level security for financial data processing."
   ```

2. **Frontend Request**: Enhanced chat interface calls the async API endpoint:
   ```typescript
   // frontend/components/chat/enhanced-chat-interface.tsx:145-176
   const handleSendMessage = async (content: string) => {
     const response = await fetch(`${apiBaseUrl}/api/chat/async`, {
       method: "POST",
       headers: { "Content-Type": "application/json" },
       body: JSON.stringify({
         messages: [{ role: "user", content: content.trim() }]
       })
     });
   };
   ```

3. **API Job Creation**: Backend (`api/main.py:285-354` - `/api/chat/async` endpoint) immediately:
   - Generates unique thread_id if not provided
   - Creates decision set with thread_id and user prompt
   - Creates job in database job queue with payload
   - Returns job metadata without waiting for processing
   ```python
   # api/main.py:297-318
   thread_id = req.thread_id or str(uuid.uuid4())
   decision_set = create_decision_set_for_thread(db, thread_id, user_prompt)

   job = job_service.create_job(
       decision_set_id=decision_set.id,
       job_type="ml_workflow",
       payload={
           "thread_id": thread_id,
           "messages": [msg.model_dump() for msg in req.messages]
       }
   )
   ```

4. **API Response**: Returns job metadata immediately (non-blocking):
   ```json
   {
     "decision_set_id": "f1262365-7421-4b84-ba42-86e05e1a729c",
     "job_id": "e3520753-94d4-4b79-9dcf-edce0cf4773a",
     "thread_id": "8c15cebc-ee52-408f-82f3-1c8c0b03f470",
     "status": "pending"
   }
   ```

5. **Frontend Response Handling**: Chat interface immediately:
   - Adds user message to chat
   - Creates system message with job details
   - Starts job status polling every 3 seconds
   - Shows "ðŸš€ Started MLOps architecture design workflow!" message
   ```typescript
   // frontend/components/chat/enhanced-chat-interface.tsx:188-216
   const systemMessage: EnhancedMessage = {
     id: (Date.now() + 1).toString(),
     role: "assistant",
     content: `ðŸš€ Started MLOps architecture design workflow! You can track real-time progress in the panel below.`,
     decisionSetId: decision_set_id,
     jobId: job_id,
     jobStatus: status,
     isStreamingActive: true
   };

   // Start job status polling
   jobPollingIntervalRef.current = setInterval(() => {
     pollJobStatus(job_id, decision_set_id);
   }, 3000);
   ```

### Phase 2: SSE Connection Establishment

6. **Automatic SSE Connection**: Frontend (`useStreamingEvents` hook) automatically establishes SSE connection:
   ```typescript
   // frontend/hooks/useStreamingEvents.ts:368-405
   const connect = useCallback((targetDecisionSetId?: string) => {
     const dsId = targetDecisionSetId || decisionSetId;
     const apiBaseUrl = getApiBaseUrl();
     const url = `${apiBaseUrl}/api/streams/${dsId}`;

     const eventSource = new EventSource(url);
     eventSourceRef.current = eventSource;

     eventSource.onopen = () => {
       console.log("SSE connection opened");
       reconnectCountRef.current = 0;
       setState(prevState => ({
         ...prevState,
         isConnected: true,
         isConnecting: false,
         connectionError: false
       }));
     };
   }, [decisionSetId, reconnectAttempts, reconnectDelay, processStreamEvent]);
   ```

7. **SSE Endpoint Processing**: Backend SSE endpoint (`api/main.py` - `/api/streams/{decision_set_id}`):
   ```python
   # api/main.py - stream_workflow_progress function
   async def event_generator():
       streaming_service = get_streaming_service()

       # Subscribe to streaming service for this decision set
       async for event in streaming_service.subscribe(decision_set_id):
           # Build payload expected by frontend
           payload = {
               "type": event.event_type.value,
               "decision_set_id": event.decision_set_id,
               "timestamp": event.timestamp.isoformat(),
               "data": event.data,
           }
           if event.message:
               payload["message"] = event.message

           # Format for sse_starlette with named events
           yield {
               "event": event.event_type.value,
               "data": json.dumps(jsonable_encoder(payload))
           }
   ```

8. **Connection Features**:
   - Verifies decision set exists in database before establishing connection
   - Registers client with in-memory streaming service
   - Supports automatic reconnection with exponential backoff (max 3 attempts)
   - Replays historical events for new connections (up to 1000 events stored)
   - Handles connection errors and cleanup gracefully
   - Named event listeners for different event types (reason-card, node-start, etc.)

### Phase 3: Integrated Worker Job Processing

9. **Integrated Worker Startup**: The integrated worker service starts with the API server:
   ```python
   # api/main.py - startup event
   @app.on_event("startup")
   async def startup_event():
       global _worker_service
       _worker_service = IntegratedWorkerService()
       await _worker_service.start_background_worker()
       logger.info("Integrated API + Worker server started successfully")
   ```

10. **Worker Loop**: Continuous job processing in background task:
    ```python
    # api/main.py - IntegratedWorkerService.worker_loop
    async def worker_loop(self):
        while self.running:
            job_processed = await self.process_next_job()
            if job_processed:
                await asyncio.sleep(1)  # Short delay between jobs
            else:
                await asyncio.sleep(self.poll_interval)  # Wait for new jobs (5s default)
    ```

11. **Job Claiming**: Worker claims jobs using database-level locking:
    ```python
    # api/main.py - process_next_job
    with self.get_job_service() as job_service:
        job = job_service.claim_job(self.worker_id, self.lease_duration)
        if job:
            logger.info(f"Claimed job {job.id} of type {job.job_type}")
            if job.job_type == "ml_workflow":
                await self.process_ml_workflow_job(job)
    ```

12. **LangGraph Workflow Initialization**: Worker processes ML workflow jobs:
    ```python
    # api/main.py - process_ml_workflow_job
    async def process_ml_workflow_job(self, job: Job):
        # Extract job data
        thread_id = job.payload.get("thread_id")
        decision_set_id = job.decision_set_id
        messages = job.payload.get("messages", [])

        # Generate run_id for LangSmith correlation
        run_id = str(uuid.uuid4())

        # Convert messages to LangChain format
        lc_messages = []
        for msg_data in messages:
            role = msg_data.get("role")
            content = msg_data.get("content", "")
            if role == "user":
                lc_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                lc_messages.append(AIMessage(content=content))

        state = {"messages": lc_messages, "decision_set_id": decision_set_id}

        # Configure LangGraph with run_id for LangSmith tracing
        config = {
            "configurable": {
                "thread_id": thread_id,
                "run_id": run_id,  # For LangSmith trace correlation
            }
        }
    ```

### Phase 4: LangGraph Streaming & Event Processing

13. **LangGraph Execution**: Worker executes graph with streaming:
    ```python
    # api/main.py - process_ml_workflow_job continued
    try:
        # Get streaming service for direct event emission
        streaming_service = get_streaming_service()

        # Execute graph with streaming enabled
        result = await self.graph.ainvoke(state, config)

        # Emit workflow completion
        await streaming_service.emit_workflow_complete(
            decision_set_id,
            final_outputs=result
        )

        logger.info(f"Workflow completed for decision_set: {decision_set_id}")

    except Exception as e:
        await streaming_service.emit_error(
            decision_set_id,
            f"Workflow failed: {str(e)}"
        )
    ```

14. **Node Start Events**: When each agent node begins execution (within LangGraph):
    ```python
    # libs/graph.py - Node execution with streaming
    def intake_extract_enhanced(state: MLOpsWorkflowState) -> MLOpsWorkflowState:
        streaming_service = get_streaming_service()
        decision_set_id = state.get("decision_set_id")

        # Emit node start event
        asyncio.create_task(streaming_service.emit_node_start(
            decision_set_id,
            "intake_extract",
            "Extracting and structuring project requirements"
        ))
    ```

15. **SSE Event Transmission**: Events stream immediately to connected clients:
    ```
    event: node-start
    data: {
      "type": "node-start",
      "decision_set_id": "f1262365-7421-4b84-ba42-86e05e1a729c",
      "timestamp": "2025-09-19T23:52:54.236Z",
      "data": {
        "node": "intake_extract",
        "message": "Extracting and structuring project requirements"
      }
    }
    ```

16. **Agent LLM Processing**: Agents execute with OpenAI/Claude (with LangSmith tracing):
    ```python
    # libs/llm_agent_base.py - Agent execution
    async def execute(self, state: MLOpsWorkflowState, trigger: TriggerType):
        # LLM call with automatic LangSmith tracing
        response = await self.llm_client.call(
            messages=formatted_messages,
            model=self.model_name
        )
        # Process and structure the response
    ```

17. **Reason Card Generation**: Agents create detailed reasoning information:
    ```python
    # Reason cards generated during agent processing
    reason_cards = [
        {
            "agent": "intake.extract",
            "node": "intake_extract_agent",
            "reasoning": "The user explicitly requests enterprise-grade image classification...",
            "decision": "Extracted constraints with 0.68 confidence",
            "confidence": 0.68,
            "category": "requirement-extraction",
            "priority": "medium",
            "inputs": {"user_input_summary": "..."},
            "outputs": {"constraints": {...}}
        }
    ]
    ```

18. **Event Broadcasting**: Streaming service broadcasts events to all connected clients:
    ```python
    # libs/streaming_service.py:36-60
    async def emit_event(self, event: StreamEvent) -> None:
        decision_set_id = event.decision_set_id

        # Store event in memory
        if decision_set_id not in self._events:
            self._events[decision_set_id] = []
        self._events[decision_set_id].append(event)

        # Limit event history to prevent memory growth (1000 -> 500)
        if len(self._events[decision_set_id]) > 1000:
            self._events[decision_set_id] = self._events[decision_set_id][-500:]

        # Broadcast to connected clients
        await self._broadcast_event(decision_set_id, event)
    ```

### Phase 5: Frontend Real-time UI Updates

19. **SSE Event Reception**: Frontend hook processes different event types:
    ```typescript
    // frontend/hooks/useStreamingEvents.ts:148-365
    const processStreamEvent = useCallback((event: StreamEvent) => {
      setState(prevState => {
        const newState = { ...prevState };

        // Add event to history (limit to 1000 events, then keep last 500)
        newState.events = [...prevState.events, event];
        if (newState.events.length > 1000) {
          newState.events = newState.events.slice(-500);
        }

        switch (event.type) {
          case 'node-start':
            newState.currentNode = event.data?.node as string;
            if (!newState.workflowProgress) {
              newState.workflowProgress = {
                current_node: event.data?.node as string,
                nodes_completed: [],
                nodes_remaining: [],
                progress_percentage: 10,
                status: "running"
              };
            }
            break;

          case 'reason-card':
            const reasonCard = event.data as ReasonCard;
            newState.reasonCards = [...prevState.reasonCards, reasonCard];
            break;

          case 'node-complete':
            const completedNode = event.data?.node as string;
            if (newState.workflowProgress) {
              const nodesCompleted = [...newState.workflowProgress.nodes_completed];
              if (!nodesCompleted.includes(completedNode)) {
                nodesCompleted.push(completedNode);
              }
              newState.workflowProgress = {
                ...newState.workflowProgress,
                nodes_completed: nodesCompleted,
                current_node: undefined,
                progress_percentage: Math.min(90, nodesCompleted.length * 20)
              };
            }
            break;
        }

        return newState;
      });
    }, []);
    ```

20. **Event Listeners Setup**: Hook sets up typed event listeners for all event types:
    ```typescript
    // frontend/hooks/useStreamingEvents.ts:440-480
    const eventTypes = [
      "reason-card", "node-start", "node-complete",
      "workflow-start", "workflow-complete", "workflow-paused",
      "error", "heartbeat", "questions-presented",
      "auto-approving", "responses-collected", "workflow-resumed", "countdown-tick"
    ];

    eventTypes.forEach(eventType => {
      eventSource.addEventListener(eventType, (event) => {
        try {
          if (!event.data) {
            console.warn(`${eventType} event has no data, skipping`);
            return;
          }
          const data = JSON.parse(event.data);
          processStreamEvent(data as StreamEvent);
        } catch (parseError) {
          console.error(`Failed to parse ${eventType} event:`, parseError);
        }
      });
    });
    ```

21. **Workflow Container Processing**: `WorkflowContainer` builds steps from events:
    ```typescript
    // frontend/components/streaming/workflow-container.tsx:57-89
    useEffect(() => {
      const processedSteps = new Map<string, WorkflowStepData>();

      // Process node start/complete events to create steps
      events.forEach(event => {
        if (event.type === 'node-start') {
          const nodeId = event.data.node as string;
          const stepTitle = getStepTitle(nodeId); // Human-readable title

          processedSteps.set(`${nodeId}-reasoning`, {
            id: `${nodeId}-reasoning`,
            title: stepTitle,
            status: 'running',
            startTime: event.timestamp,
            stepType: 'reasoning'
          });
        }

        if (event.type === 'node-complete') {
          const nodeId = event.data.node as string;
          const reasoningStep = processedSteps.get(`${nodeId}-reasoning`);
          if (reasoningStep) {
            processedSteps.set(`${nodeId}-reasoning`, {
              ...reasoningStep,
              status: 'completed',
              endTime: event.timestamp
            });
          }
        }
      });

      // Attach reason cards to reasoning steps
      reasonCards.forEach(reasonCard => {
        const nodeId = reasonCard.node;
        const reasoningStepId = `${nodeId}-reasoning`;
        const existing = processedSteps.get(reasoningStepId);
        if (existing) {
          processedSteps.set(reasoningStepId, { ...existing, reasonCard });
        }
      });

      setSteps(Array.from(processedSteps.values()));
    }, [events, reasonCards]);
    ```

22. **Real-time UI Rendering**: Components update automatically:
    - **Progress Header**: "MLOps Workflow Progress - 40% Complete"
    - **Status Badge**: "Processing" with animated indicator
    - **Current Step**: "Analyzing requirements and selecting MLOps patterns"
    - **Completed Steps**: Auto-collapsed with checkmark icons
    - **Expandable Reason Cards**: Click to view detailed agent reasoning
    - **Confidence Indicators**: Visual badges showing agent confidence levels
    - **Agent Metadata**: Timing, inputs, outputs, and alternatives considered

### Phase 6: Workflow Completion & Cleanup

23. **Workflow Completion Detection**: Backend detects when graph execution finishes:
    ```python
    # api/main.py - process_ml_workflow_job completion
    try:
        result = await self.graph.ainvoke(state, config)

        # Emit workflow completion event
        await streaming_service.emit_workflow_complete(
            decision_set_id,
            final_outputs=result
        )

        logger.info(f"Workflow completed for decision_set: {decision_set_id}")

    except Exception as e:
        await streaming_service.emit_error(
            decision_set_id,
            f"Workflow failed: {str(e)}"
        )
        raise
    ```

24. **Job Status Update**: Worker marks job as completed in database:
    ```python
    # api/main.py - process_next_job completion
    finally:
        # Always attempt to complete the job
        with self.get_job_service() as job_service:
            success = job_service.complete_job(job.id, self.worker_id)
            if success:
                logger.info(f"Successfully completed job {job.id}")
            else:
                logger.warning(f"Failed to complete job {job.id}")
    ```

25. **Frontend Job Polling**: Chat interface polls for job completion:
    ```typescript
    // frontend/components/chat/enhanced-chat-interface.tsx:60-143
    const pollJobStatus = async (jobId: string, decisionSetId: string) => {
      try {
        const response = await fetch(`${apiBaseUrl}/api/jobs/${jobId}/status`);
        const jobStatus = await response.json();

        // Update message with job status
        setChatState(prev => ({
          ...prev,
          messages: prev.messages.map(msg =>
            msg.jobId === jobId
              ? {
                  ...msg,
                  jobStatus: jobStatus.status,
                  isStreamingActive: jobStatus.status === "processing"
                }
              : msg
          )
        }));

        // Stop polling if job is complete or failed
        if (jobStatus.status === "completed" || jobStatus.status === "failed") {
          clearInterval(jobPollingIntervalRef.current);
          jobPollingIntervalRef.current = null;

          if (jobStatus.status === "completed") {
            // Add completion message if not already added
            setChatState(prev => {
              const hasCompletionMessage = prev.messages.some(msg =>
                msg.content.includes("MLOps architecture design completed") &&
                msg.decisionSetId === decisionSetId
              );

              if (!hasCompletionMessage) {
                return {
                  ...prev,
                  messages: [...prev.messages, {
                    id: Date.now().toString(),
                    role: "assistant",
                    content: "âœ… MLOps architecture design completed! The workflow has finished successfully.",
                    timestamp: new Date(),
                    decisionSetId
                  } as EnhancedMessage]
                };
              }
              return prev;
            });
          }
        }
      } catch (error) {
        console.error("Job status polling failed:", error);
        clearInterval(jobPollingIntervalRef.current);
        jobPollingIntervalRef.current = null;
      }
    };
    ```

26. **SSE Connection Lifecycle**: Frontend manages connection cleanup:
    ```typescript
    // frontend/hooks/useStreamingEvents.ts:538-546
    useEffect(() => {
      return () => {
        clearReconnectTimeout();
        if (eventSourceRef.current) {
          eventSourceRef.current.close();
        }
      };
    }, [clearReconnectTimeout]);

    // Auto-connect and cleanup
    useEffect(() => {
      if (autoConnect && decisionSetId) {
        connect(decisionSetId);
      }
      return () => {
        disconnect();
      };
    }, [autoConnect, decisionSetId, connect, disconnect]);
    ```

27. **Backend Connection Cleanup**: Streaming service manages memory and connections:
    ```python
    # libs/streaming_service.py - _broadcast_event method handles cleanup
    async def _broadcast_event(self, decision_set_id: str, event: StreamEvent):
        if decision_set_id not in self._connections:
            return

        connections = self._connections[decision_set_id][:]
        for queue in connections:
            try:
                await queue.put(event)
            except Exception as e:
                logger.warning(f"Failed to send event to client: {e}")
                # Remove failed connection
                if queue in self._connections[decision_set_id]:
                    self._connections[decision_set_id].remove(queue)
    ```

28. **Event History Management**: Service maintains event history with automatic cleanup:
    ```python
    # libs/streaming_service.py:50-53
    # Limit event history to prevent memory growth
    if len(self._events[decision_set_id]) > 1000:
        self._events[decision_set_id] = self._events[decision_set_id][-500:]
        logger.info(f"Cleaned up old events for decision_set: {decision_set_id}")
    ```

### Key Implementation Details

**Integrated Worker Architecture**: Single process for API and job processing:
```python
# api/main.py - Startup event
@app.on_event("startup")
async def startup_event():
    global _worker_service
    _worker_service = IntegratedWorkerService(
        worker_id=f"worker-{uuid.uuid4().hex[:8]}",
        poll_interval=5,
        lease_duration=30
    )
    await _worker_service.start_background_worker()
    logger.info("Integrated API + Worker server started successfully")
```

**Direct Streaming Service Access**: No HTTP bridge needed:
```python
# Direct access within same process
from libs.streaming_service import get_streaming_service
streaming_service = get_streaming_service()

# Emit events directly from worker
await streaming_service.emit_reason_card(reason_card)
await streaming_service.emit_node_start(decision_set_id, node_name)
```

**LangSmith Integration**: Full observability with run_id correlation:
```python
# Generate run_id for correlation
run_id = str(uuid.uuid4())

# Structured logging with correlation fields
logger.info(
    f"Processing ML workflow for thread {thread_id}, decision_set {decision_set_id}",
    extra={"run_id": run_id, "thread_id": thread_id, "decision_set_id": decision_set_id, "job_id": job.id}
)

# LangGraph config with run_id for LangSmith tracing
config = {
    "configurable": {
        "thread_id": thread_id,
        "run_id": run_id,  # Enables trace correlation
    }
}
```

**Single Graph Execution**: Standard LangGraph invocation with streaming events:
```python
# api/main.py - Standard graph execution with events
try:
    result = await self.graph.ainvoke(state, config)

    await streaming_service.emit_workflow_complete(
        decision_set_id,
        final_outputs=result
    )
except Exception as e:
    await streaming_service.emit_error(
        decision_set_id,
        f"Workflow failed: {str(e)}"
    )
```

**Single Workflow Container**: Frontend shows one container per decision set:
```typescript
// frontend/components/chat/enhanced-chat-interface.tsx:297-302
const isFirstMessageWithDecisionSetId =
  message.role === "assistant" && message.decisionSetId &&
  !chatState.messages.slice(0, index).some(m =>
    m.decisionSetId === message.decisionSetId
  );

// Show workflow container only for first message with each decisionSetId
{isFirstMessageWithDecisionSetId && (
  <div className="ml-4">
    <WorkflowContainer
      decisionSetId={message.decisionSetId}
      className="mt-3"
    />
  </div>
)}
```

**SSE Resilience**: Automatic reconnection with exponential backoff:
```typescript
// frontend/hooks/useStreamingEvents.ts:410-437
eventSource.onerror = (error) => {
  console.error("SSE connection error:", error);
  setState(prevState => ({
    ...prevState,
    isConnected: false,
    isConnecting: false,
    connectionError: true
  }));

  // Attempt reconnection if within limits
  if (reconnectCountRef.current < reconnectAttempts) {
    reconnectCountRef.current++;
    console.log(`Attempting to reconnect (${reconnectCountRef.current}/${reconnectAttempts})`);

    clearReconnectTimeout();
    reconnectTimeoutRef.current = setTimeout(() => {
      connect(dsId);
    }, reconnectDelay);
  } else {
    console.error("Max reconnection attempts reached");
    setState(prevState => ({
      ...prevState,
      error: "Connection failed after multiple attempts"
    }));
  }
};
```

## Backend Implementation

### Core Components

#### 1. Streaming Models (`libs/streaming_models.py`)

```python
class StreamEventType(str, Enum):
    REASON_CARD = "reason-card"
    NODE_START = "node-start"
    NODE_COMPLETE = "node-complete"
    WORKFLOW_START = "workflow-start"
    WORKFLOW_COMPLETE = "workflow-complete"
    ERROR = "error"
    HEARTBEAT = "heartbeat"
    WORKFLOW_PAUSED = "workflow-paused"

class ReasonCard(BaseModel):
    agent: str
    node: str
    decision_set_id: str
    reasoning: str
    decision: str
    confidence: Optional[float]
    inputs: Optional[Dict[str, Any]]
    outputs: Optional[Dict[str, Any]]
    alternatives_considered: Optional[list[str]]
    category: str
    priority: str
```

#### 2. Streaming Service (`libs/streaming_service.py`)

**Key Features:**
- In-memory event storage with automatic limits (1000 events â†’ 500 after cleanup)
- Connection management with asyncio queues
- Historical event replay for new connections
- Heartbeat mechanism to keep connections alive

**Core Methods:**
```python
async def emit_event(self, event: StreamEvent) -> None:
    # Store event and broadcast to connected clients

async def subscribe(self, decision_set_id: str) -> AsyncGenerator[StreamEvent, None]:
    # Handle client subscriptions with historical replay

async def emit_reason_card(self, reason_card: ReasonCard) -> None:
    # Emit structured agent reasoning events
```

#### 3. Integrated Worker & LangGraph Processing

The integrated worker processes jobs and streams events directly:

```python
# api/main.py - IntegratedWorkerService
class IntegratedWorkerService:
    """Integrated worker that runs in the same process as the API server."""

    async def process_ml_workflow_job(self, job: Job):
        """Process ML workflow with direct streaming access."""
        # Extract job data
        thread_id = job.payload.get("thread_id")
        decision_set_id = job.decision_set_id
        messages = job.payload.get("messages", [])

        # Generate run_id for LangSmith correlation
        run_id = str(uuid.uuid4())

        # Configure LangGraph with observability
        config = {
            "configurable": {
                "thread_id": thread_id,
                "run_id": run_id,  # LangSmith tracing
            }
        }

        # Direct streaming service access (no HTTP needed)
        streaming_service = get_streaming_service()

        try:
            # Multi-mode streaming for comprehensive insights
            stream_modes = ["updates", "messages"]

            async for stream_mode, chunk in self.graph.astream(
                state, config, stream_mode=stream_modes
            ):
                if stream_mode == "updates":
                    await self.process_updates_stream(chunk, decision_set_id)
                elif stream_mode == "messages":
                    await self.process_messages_stream(chunk, decision_set_id)

        except Exception as e:
            await streaming_service.emit_error(
                decision_set_id, f"Workflow failed: {str(e)}"
            )
            raise

    async def process_updates_stream(self, chunk, decision_set_id):
        """Process LangGraph updates stream for node events and reason cards."""
        streaming_service = get_streaming_service()

        for node_name, node_data in chunk.items():
            # Emit node start
            await streaming_service.emit_node_start(
                decision_set_id, node_name, f"Processing {node_name}"
            )

            # Process and emit reason cards with deduplication
            reason_cards = node_data.get("reason_cards", [])
            unique_cards = self.deduplicate_reason_cards(reason_cards)

            for reason_card in unique_cards:
                # Convert dict to ReasonCard if needed
                if isinstance(reason_card, dict):
                    card = create_reason_card(
                        agent=reason_card.get("agent", "unknown"),
                        node=reason_card.get("node", "unknown"),
                        decision_set_id=decision_set_id,
                        reasoning=reason_card.get("reasoning", ""),
                        decision=reason_card.get("decision", ""),
                        confidence=reason_card.get("confidence", 0.5),
                        category=reason_card.get("category", "unknown"),
                        priority=reason_card.get("priority", "medium")
                    )
                else:
                    card = reason_card

                await streaming_service.emit_reason_card(card)

            # Emit node completion
            await streaming_service.emit_node_complete(
                decision_set_id, node_name, node_data.get("outputs", {})
            )
```

#### 4. SSE API Endpoint (`api/main.py`)

```python
@app.get("/api/streams/{decision_set_id}")
async def stream_workflow_progress(decision_set_id: str, db: Session = Depends(get_db)):
    """Stream real-time workflow progress via Server-Sent Events."""
    from sse_starlette.sse import EventSourceResponse
    from libs.streaming_service import get_streaming_service

    # Verify decision set exists
    decision_set = db.query(DecisionSet).filter(
        DecisionSet.id == decision_set_id
    ).first()
    if not decision_set:
        raise HTTPException(status_code=404, detail="Decision set not found")

    streaming_service = get_streaming_service()

    async def event_generator():
        """Generate properly formatted SSE events for frontend consumption."""
        try:
            logger.info(f"Starting SSE event generator for: {decision_set_id}")

            async for event in streaming_service.subscribe(decision_set_id):
                # Build payload expected by frontend hook
                payload = {
                    "type": event.event_type.value,
                    "decision_set_id": event.decision_set_id,
                    "timestamp": event.timestamp.isoformat(),
                    "data": event.data,
                }
                if event.message:
                    payload["message"] = event.message

                # Format for sse_starlette with named events
                yield {
                    "event": event.event_type.value,
                    "data": json.dumps(jsonable_encoder(payload))
                }

        except asyncio.CancelledError:
            logger.info(f"SSE connection cancelled: {decision_set_id}")
            raise
        except Exception as e:
            logger.exception(f"SSE stream error: {decision_set_id}")
            await streaming_service.emit_error(
                decision_set_id, f"Stream error: {str(e)}"
            )

    return EventSourceResponse(
        event_generator(),
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )
```

### Event Flow in Backend

1. **Integrated Worker**: Claims jobs from database queue using `FOR UPDATE SKIP LOCKED`
2. **LangGraph Execution**: Multi-mode streaming (`updates` + `messages`) provides rich data
3. **Direct Event Emission**: Worker emits events directly to streaming service (no HTTP bridge)
4. **Streaming Service**: Central hub that stores events in memory and broadcasts to SSE clients
5. **SSE Endpoint**: Provides EventSource connection for real-time streaming to frontend
6. **Connection Management**: Handles multiple clients, heartbeats, cleanup, and reconnection
7. **LangSmith Integration**: All runs automatically traced with run_id correlation
8. **Structured Logging**: CloudWatch-compatible logs with run_id/thread_id correlation

## Frontend Implementation

### Core Components

#### 1. SSE Hook (`hooks/useStreamingEvents.ts`)

**Key Features:**
- Manages SSE connection lifecycle with automatic reconnection
- Real-time event processing and React state management
- Connection resilience with exponential backoff
- Event deduplication and memory management

**Usage:**
```typescript
const {
  isConnected,
  events,
  reasonCards,
  workflowProgress,
  isWorkflowActive,
  isWorkflowComplete
} = useStreamingEvents({
  decisionSetId: "f1262365-7421-4b84-ba42-86e05e1a729c",
  autoConnect: true,
  reconnectAttempts: 3,
  reconnectDelay: 2000
});
```

**Event Processing:**
```typescript
// Processes incoming SSE events and updates React state
const processStreamEvent = useCallback((event: StreamEvent) => {
  switch (event.type) {
    case 'reason-card':
      setReasonCards(prev => [...prev, event.data as ReasonCard]);
      break;
    case 'node-start':
      setCurrentNode(event.data.node);
      setWorkflowProgress(prev => ({ ...prev, current_node: event.data.node }));
      break;
    case 'node-complete':
      setWorkflowProgress(prev => ({
        ...prev,
        nodes_completed: [...prev.nodes_completed, event.data.node],
        progress_percentage: Math.min(90, prev.nodes_completed.length * 20)
      }));
      break;
  }
}, []);
```

#### 2. Workflow Container (`components/streaming/workflow-container.tsx`)

**Claude-style Unified Interface:**
- Single workflow display per decision set (no duplicate containers)
- Auto-collapsing completed steps to focus on current activity
- Progress header with percentage and status badges
- Collapsible workflow steps with reasoning details

**Step Processing:**
```typescript
// Creates unified workflow steps from events and reason cards
useEffect(() => {
  const processedSteps = new Map<string, WorkflowStepData>();

  // Process node events to create reasoning steps
  events.forEach(event => {
    if (event.type === 'node-start') {
      const nodeId = event.data.node as string;
      const stepTitle = getStepTitle(nodeId); // Human-readable title

      processedSteps.set(`${nodeId}-reasoning`, {
        id: `${nodeId}-reasoning`,
        title: stepTitle,
        status: 'running',
        startTime: event.timestamp,
        stepType: 'reasoning',
      });
    }
  });

  // Attach reason cards to steps
  reasonCards.forEach(reasonCard => {
    const stepId = `${reasonCard.node}-reasoning`;
    const existing = processedSteps.get(stepId);
    if (existing) {
      processedSteps.set(stepId, { ...existing, reasonCard });
    }
  });
}, [events, reasonCards]);
```

#### 3. Workflow Step (`components/streaming/workflow-step.tsx`)

**Collapsible Reasoning Display:**
- Brain icon indicator for reasoning steps
- Expandable/collapsible with chevron indicators
- Rich reason card content when expanded
- Status-based visual styling

**Reason Card Content:**
- **Reasoning Section**: Detailed agent explanation
- **Decision Section**: Key decision made by agent
- **Confidence & Priority**: Visual badges with percentages
- **Alternatives Considered**: List of options the agent evaluated
- **Agent Metadata**: Agent name, node, and timing information

#### 4. Enhanced Chat Interface (`components/chat/enhanced-chat-interface.tsx`)

**Integrated Streaming Features:**
- Calls `/api/chat/async` for non-blocking job creation
- Automatic SSE connection establishment
- Inline workflow display (not separate panel)
- Job status polling with 3-second intervals
- Single WorkflowContainer per unique decisionSetId

**Message Flow:**
```typescript
// Sends message and establishes streaming
const handleSendMessage = async (content: string) => {
  // 1. Send async API request
  const response = await fetch(`${apiBaseUrl}/api/chat/async`, {
    method: "POST",
    body: JSON.stringify({ messages: [{ role: "user", content }] })
  });

  const { decision_set_id, job_id } = await response.json();

  // 2. Create system message with job details
  const systemMessage: EnhancedMessage = {
    role: "assistant",
    content: "ðŸš€ Started MLOps architecture design workflow!...",
    decisionSetId: decision_set_id,
    jobId: job_id,
    isStreamingActive: true
  };

  // 3. Start job status polling
  jobPollingInterval = setInterval(() => {
    pollJobStatus(job_id, decision_set_id);
  }, 3000);
};
```

**Single Container Logic:**
```typescript
// Shows WorkflowContainer only for first message with each decisionSetId
{chatState.messages.map((message, index) => {
  const isFirstMessageWithDecisionSetId =
    message.role === "assistant" && message.decisionSetId &&
    !chatState.messages.slice(0, index).some(m =>
      m.decisionSetId === message.decisionSetId
    );

  return (
    <div key={message.id}>
      {/* Message display */}
      <MessageCard content={message.content} />

      {/* Single workflow container per decision set */}
      {isFirstMessageWithDecisionSetId && (
        <WorkflowContainer decisionSetId={message.decisionSetId} />
      )}
    </div>
  );
})}
```

### Frontend Event Processing

1. **Connection Establishment**: SSE connection to backend endpoint
2. **Event Reception**: Real-time event processing with type safety
3. **State Updates**: React state updates trigger UI re-renders
4. **Component Updates**: Specialized components display different event types
5. **Connection Resilience**: Automatic reconnection on connection loss

## Data Models & Types

### Backend Models (Python)

```python
# libs/streaming_models.py
class StreamEventType(str, Enum):
    REASON_CARD = "reason-card"
    NODE_START = "node-start"
    NODE_COMPLETE = "node-complete"
    WORKFLOW_START = "workflow-start"
    WORKFLOW_COMPLETE = "workflow-complete"
    ERROR = "error"
    HEARTBEAT = "heartbeat"
    WORKFLOW_PAUSED = "workflow-paused"
    QUESTIONS_PRESENTED = "questions-presented"
    AUTO_APPROVING = "auto-approving"
    RESPONSES_COLLECTED = "responses-collected"
    WORKFLOW_RESUMED = "workflow-resumed"
    COUNTDOWN_TICK = "countdown-tick"

# Stream Event Structure
class StreamEvent(BaseModel):
    event_type: StreamEventType
    decision_set_id: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    data: Dict[str, Any] = Field(default_factory=dict)
    message: Optional[str] = None

# Reason Card Structure
class ReasonCard(BaseModel):
    agent: str                                    # Agent identifier (e.g., "intake.extract")
    node: str                                     # Graph node name (e.g., "intake_extract_agent")
    decision_set_id: str                         # Workflow session ID
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    duration_ms: Optional[int] = None            # Processing time in milliseconds
    reasoning: str                               # Human-readable explanation
    decision: str                                # Key decision made
    confidence: Optional[float] = None           # Confidence (0.0-1.0)
    category: str                                # Decision category
    priority: str = "medium"                     # Priority level (low/medium/high)
    inputs: Optional[Dict[str, Any]] = None      # Input data
    outputs: Optional[Dict[str, Any]] = None     # Output results
    alternatives_considered: Optional[List[str]] = None  # Alternative options
    references: Optional[List[str]] = None       # References used
```

### Frontend Types (TypeScript)

```typescript
// Event Types
type StreamEventType = 
  | 'reason-card' | 'node-start' | 'node-complete'
  | 'workflow-start' | 'workflow-complete' 
  | 'error' | 'heartbeat' | 'workflow-paused';

// Streaming State
interface StreamingState {
  isConnected: boolean;
  events: StreamEvent[];
  reasonCards: ReasonCard[];
  workflowProgress: WorkflowProgress | null;
  currentNode: string | null;
  error: string | null;
}

// Workflow Progress
interface WorkflowProgress {
  current_node?: string;
  nodes_completed: string[];
  nodes_remaining: string[];
  progress_percentage: number;
  status: string;
}
```

## Testing & Quality Assurance

### Backend Tests (`tests/test_streaming_functionality.py`)

**Coverage:**
- 24 comprehensive test cases
- StreamingService functionality
- Event processing and storage
- Connection management
- SSE formatting
- Integration testing

**Key Test Categories:**
```python
class TestStreamingService:
    def test_emit_event()                    # Basic event emission
    def test_emit_reason_card()              # Reason card streaming
    def test_subscription_with_historical_events()  # Event replay
    def test_event_limit_enforcement()       # Memory management
    def test_connection_count_tracking()     # Connection monitoring
```

### Frontend Quality Assurance

**TypeScript Compilation:**
- âœ… Strict type checking passes
- âœ… All components type-safe
- âœ… Full IntelliSense support

**Build Verification:**
- âœ… Production build successful
- âœ… Development server starts without errors
- âœ… Component integration verified

## Deployment & Configuration

### Environment Variables

**Backend:**
- `GRAPH_TYPE`: Graph type to use (`full`, `streaming_test`, `hitl`, `hitl_enhanced`)
- `LOG_LEVEL`: Logging verbosity (default: INFO)
- `ENVIRONMENT`: Environment setting (`development`, `production`)
- `DATABASE_URL`: Database connection string (auto-configured for development)
- `OPENAI_API_KEY`: OpenAI API key for LLM agent reasoning
- `ANTHROPIC_API_KEY`: Claude API key for code generation
- `LANGCHAIN_TRACING_V2`: Enable LangSmith tracing (true/false)
- `LANGCHAIN_API_KEY`: LangSmith API key for observability
- `LANGCHAIN_PROJECT`: LangSmith project name

**Frontend:**
- `NEXT_PUBLIC_API_BASE_URL`: Backend API URL (default: http://localhost:8000)

### Production Considerations

1. **Integrated Architecture Benefits**:
   - Single process deployment simplifies container orchestration
   - Direct streaming service access eliminates HTTP overhead
   - Shared memory space enables efficient event processing
   - Simplified debugging and monitoring

2. **Memory Management**:
   - Event storage limited to 1000 events per decision set
   - Automatic cleanup to 500 events when limit reached
   - Connection cleanup on workflow completion
   - Job queue cleanup using database retention policies

3. **Observability**:
   - LangSmith integration for full LangGraph trace visibility
   - Structured logging with run_id/thread_id correlation
   - CloudWatch-compatible JSON logs in production
   - Event history replay for debugging

4. **Scalability**:
   - Current design suitable for single-instance deployment
   - For multi-instance: consider Redis-backed streaming service
   - Database job queue scales with `FOR UPDATE SKIP LOCKED`
   - Connection pooling and load balancing considerations

5. **Error Handling**:
   - Graceful degradation when SSE connection fails
   - Automatic reconnection with exponential backoff
   - Job retry logic with failure tracking
   - User-friendly error messages and recovery

6. **Security**:
   - CORS configuration for production domains
   - JWT authentication integration ready
   - Input validation on all endpoints
   - Database-level security with prepared statements

## Usage Examples

### Real Example: Image Classification Platform

1. **User Input**: "I need an ML platform for image classification with high availability requirements and enterprise-level security for financial data processing."

2. **API Response**:
```json
{
  "decision_set_id": "3856ebd4-81fe-479c-8f29-57e5756c91a1",
  "job_id": "7ec73c84-1e69-48da-bfa7-292e51bd1643",
  "thread_id": "9b5426ad-a350-4eba-980b-422fac25eded",
  "status": "queued"
}
```

3. **Real SSE Events Stream**:
```
event: node-start
data: {
  "type": "node-start",
  "decision_set_id": "3856ebd4-81fe-479c-8f29-57e5756c91a1",
  "timestamp": "2025-09-19T23:52:54.236Z",
  "data": {
    "node": "intake_extract",
    "message": "Extracting and structuring project requirements"
  }
}

event: reason-card
data: {
  "type": "reason-card",
  "decision_set_id": "3856ebd4-81fe-479c-8f29-57e5756c91a1",
  "timestamp": "2025-09-19T23:52:54.184Z",
  "data": {
    "agent": "intake.extract",
    "node": "intake_extract_agent",
    "reasoning": "The user explicitly requests enterprise-grade image classification platform with high availability and security for financial data. This implies restricted data classification, compliance (SOX, PCI-DSS), and disaster recovery.",
    "decision": "Extracted constraints with 0.68 confidence",
    "confidence": 0.68,
    "category": "requirement-extraction",
    "priority": "medium",
    "inputs": {
      "user_input_summary": "ML platform for image classification with high availability requirements and enterprise-level security for financial data processing."
    },
    "outputs": {
      "constraints": {
        "data_classification": "restricted",
        "compliance_requirements": ["SOX", "PCI-DSS"],
        "deployment_preference": "serverless",
        "disaster_recovery_required": true
      }
    }
  }
}

event: node-complete
data: {
  "type": "node-complete",
  "decision_set_id": "3856ebd4-81fe-479c-8f29-57e5756c91a1",
  "timestamp": "2025-09-19T23:52:54.194Z",
  "data": {
    "node": "intake_extract",
    "outputs": {
      "constraints": {...},
      "extraction_confidence": 0.68
    }
  }
}
```

### Frontend Integration

```typescript
// Enhanced Chat Interface with Inline Workflow Display
function EnhancedChatInterface() {
  const [chatState, setChatState] = useState<ChatState>({
    messages: [],
    isLoading: false,
  });

  const handleSendMessage = async (content: string) => {
    // 1. Send async API request
    const response = await fetch(`${apiBaseUrl}/api/chat/async`, {
      method: "POST",
      body: JSON.stringify({ messages: [{ role: "user", content }] })
    });

    const { decision_set_id, job_id, thread_id } = await response.json();

    // 2. Add system message with job details
    const systemMessage: EnhancedMessage = {
      role: "assistant",
      content: "ðŸš€ Started MLOps architecture design workflow!",
      decisionSetId: decision_set_id,
      jobId: job_id,
      isStreamingActive: true
    };

    // 3. Start job status polling
    startJobPolling(job_id, decision_set_id);
  };

  return (
    <div className="flex flex-col h-full">
      {/* Chat Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {chatState.messages.map((message, index) => {
          const isFirstMessageWithDecisionSetId =
            message.role === "assistant" && message.decisionSetId &&
            !chatState.messages.slice(0, index).some(m =>
              m.decisionSetId === message.decisionSetId
            );

          return (
            <div key={message.id}>
              <MessageCard message={message} />

              {/* Single WorkflowContainer per decision set */}
              {isFirstMessageWithDecisionSetId && (
                <WorkflowContainer
                  decisionSetId={message.decisionSetId}
                  className="mt-4"
                />
              )}
            </div>
          );
        })}
      </div>

      {/* Message Input */}
      <MessageInput onSendMessage={handleSendMessage} />
    </div>
  );
}

// Workflow Container with Real-time Updates
function WorkflowContainer({ decisionSetId }: { decisionSetId: string }) {
  const {
    isConnected,
    events,
    reasonCards,
    workflowProgress,
    error,
    hitlState
  } = useStreamingEvents({
    decisionSetId,
    autoConnect: true,
    reconnectAttempts: 3,
    reconnectDelay: 2000,
  });

  return (
    <div className="border rounded-lg p-4 bg-slate-50">
      {/* Progress Header */}
      <WorkflowProgressHeader
        progress={workflowProgress}
        isConnected={isConnected}
      />

      {/* Workflow Steps */}
      {steps.map(step => (
        <WorkflowStep
          key={step.id}
          step={step}
          isCollapsed={collapsedSteps.has(step.id)}
          onToggleCollapse={() => toggleStepCollapse(step.id)}
        />
      ))}

      {/* HITL Questions */}
      {hitlState.isActive && (
        <QuestionForm
          questions={hitlState.questions}
          smartDefaults={hitlState.smartDefaults}
          onSubmit={handleHitlSubmit}
        />
      )}

      {/* Error Display */}
      {error && (
        <ErrorDisplay error={error} onRetry={() => window.location.reload()} />
      )}
    </div>
  );
}
```

## Troubleshooting

### Common Issues

1. **SSE Connection Failed**:
   - Check CORS configuration
   - Verify API base URL in frontend
   - Ensure backend streaming service is running

2. **Events Not Appearing**:
   - Verify decision_set_id matches between frontend and backend
   - Check browser network tab for SSE connection
   - Review backend logs for streaming errors

3. **Memory Issues**:
   - Monitor event storage limits
   - Check for connection leaks
   - Verify cleanup is working properly

### Debug Tools

**Backend Debugging:**
```python
# Enable detailed logging
logging.getLogger('libs.streaming_service').setLevel(logging.DEBUG)
logging.getLogger('api.main').setLevel(logging.DEBUG)

# Check active connections and events
streaming_service = get_streaming_service()
active_connections = streaming_service._connections
event_counts = {k: len(v) for k, v in streaming_service._events.items()}

logger.info(f"Active connections: {len(active_connections)}")
logger.info(f"Event counts per decision set: {event_counts}")

# Check worker status
worker_service = app.state.worker_service
logger.info(f"Worker running: {worker_service.running}")
```

**Frontend Debugging:**
```typescript
// Hook debugging
const streaming = useStreamingEvents({ decisionSetId });

console.log('Streaming state:', {
  isConnected: streaming.isConnected,
  isConnecting: streaming.isConnecting,
  eventCount: streaming.events.length,
  reasonCardCount: streaming.reasonCards.length,
  workflowProgress: streaming.workflowProgress,
  error: streaming.error,
  hitlActive: streaming.hitlState.isActive
});

// Monitor SSE connection
const eventSource = new EventSource('/api/streams/' + decisionSetId);
eventSource.onopen = () => console.log('SSE connected');
eventSource.onerror = (e) => console.error('SSE error:', e);
eventSource.onmessage = (e) => console.log('SSE message:', e.data);

// Check network tab in browser DevTools for:
// - SSE connection (should show as event-stream)
// - Named events (reason-card, node-start, etc.)
// - Proper JSON payloads
```

## Future Enhancements

### Planned Improvements

1. **Scalability Enhancements**:
   - Redis-backed streaming service for multi-instance deployment
   - Database-backed event storage for persistence
   - Horizontal scaling with load balancing
   - Event replay from persistent storage

2. **Advanced Observability**:
   - LangSmith dashboard integration
   - Custom metrics and alerting
   - Performance analytics and bottleneck detection
   - Cost tracking and optimization recommendations

3. **Enhanced User Experience**:
   - Interactive reason card comparison
   - Workflow visualization with dependency graphs
   - Real-time collaborative features
   - Export functionality for workflow traces

4. **Performance Optimizations**:
   - Event compression for large payloads
   - Selective event subscriptions based on user preferences
   - Connection pooling and multiplexing
   - Client-side caching with intelligent invalidation

5. **AI/ML Features**:
   - Agent performance scoring and recommendations
   - Workflow optimization based on historical data
   - Predictive failure detection
   - Auto-tuning of confidence thresholds

---

## Summary

This comprehensive guide documents the complete streaming architecture implementation in the Agentic MLOps platform. The system successfully delivers:

### âœ… **Key Achievements**

- **Real-time Transparency**: Users see AI agent reasoning as it happens
- **Integrated Architecture**: Single-process deployment with direct streaming
- **Full Observability**: LangSmith integration with trace correlation
- **Resilient Connections**: SSE with automatic reconnection and error handling
- **Rich Agent Insights**: Multi-mode LangGraph streaming with detailed reason cards
- **Production Ready**: Structured logging, memory management, and scalability considerations

### ðŸ”„ **End-to-End Flow Summary**

1. **User Input** â†’ Frontend sends async API request
2. **Job Creation** â†’ Backend creates database job and returns immediately
3. **SSE Connection** â†’ Frontend auto-connects to streaming endpoint
4. **Worker Processing** â†’ Integrated worker claims job and runs LangGraph
5. **Event Streaming** â†’ Real-time events flow from worker to frontend via SSE
6. **UI Updates** â†’ Frontend displays workflow steps and reason cards in real-time
7. **Completion** â†’ Job completes, connections clean up gracefully

### ðŸ—ï¸ **Architecture Benefits**

- **Simplified Deployment**: No separate worker processes or message queues
- **Low Latency**: Direct in-memory event emission without HTTP overhead
- **High Reliability**: Database job queue with lease-based processing
- **Developer Experience**: Rich debugging with LangSmith traces and structured logs
- **User Experience**: Claude-style interface with transparent AI reasoning

The implementation provides a solid foundation for transparent, real-time AI agent collaboration while maintaining excellent performance and reliability.